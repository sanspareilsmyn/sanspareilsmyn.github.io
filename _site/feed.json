{
    "version": "https://jsonfeed.org/version/1",
    "title": "Sangmin Yoon",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": null,
    "icon": "http://localhost:4000/apple-touch-icon.png",
    "favicon": "http://localhost:4000/favicon.ico",
    "expired": false,
    
    "author": "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}",
    
"items": [
    
        {
            "id": "http://localhost:4000/2025/01/27/what-i-ve-enjoyed-jan-2025",
            "title": "what i've enjoyed: jan 2025",
            "summary": "what i've enjoyed: jan 2025",
            "content_text": "Movie  Triangle of Sadness, Ruben ÖstlundMusic  The Dude, Quincy Jones  Another Green World, Brian EnoGame  Cyberpunk 2077",
            "content_html": "<h2 id=\"movie\">Movie</h2><ol>  <li><a href=\"https://en.wikipedia.org/wiki/Triangle_of_Sadness\">Triangle of Sadness</a>, Ruben Östlund</li></ol><h2 id=\"music\">Music</h2><ol>  <li><a href=\"https://en.wikipedia.org/wiki/The_Dude_(Quincy_Jones_album)\">The Dude</a>, Quincy Jones</li>  <li><a href=\"https://en.wikipedia.org/wiki/Another_Green_World\">Another Green World</a>, Brian Eno</li></ol><h2 id=\"game\">Game</h2><ol>  <li><a href=\"https://en.wikipedia.org/wiki/Cyberpunk_2077\">Cyberpunk 2077</a></li></ol>",
            "url": "http://localhost:4000/2025/01/27/what-i-ve-enjoyed-jan-2025",
            
            
            
            
            
            "date_published": "2025-01-27T00:00:00+09:00",
            "date_modified": "2025-01-27T00:00:00+09:00",
            
                "author": 
                "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2025/01/23/airflow-dag-parsing-how-volumes-and-file-changes-interact",
            "title": "Airflow DAG Parsing: How Volumes and File Changes Interact",
            "summary": "Understanding how Airflow detects DAG file changes and how it interacts with container volumes",
            "content_text": "1. IntroductionI run Airflow in a Kubernetes environment, relying on a script to pull DAG definitions from a remote Git repository. My goal was to implement domain-specific DAG deployments by dynamically managing .airflowignore files. I attempted to inject these .airflowignore files directly into the Kubernetes volume where our DAGs reside. However, this seemingly straightforward approach led to unexpected challenges, uncovering some interesting details about how Airflow interacts with mounted volumes. I explored my process and lessons learned about Airflow’s DAG parsing logic.2. ApproachesI started by considering a few ideas:1. **Modifying gitsync.sh to exclude and reset .airflowignoreThis was quickly discarded. Without a remote version of the desired .airflowignore, I couldn’t reliably restore it via command line.2. Using Airflow variables to hide DAGsI discovered that, except for .airflowignore, there is no way to prevent DAG parsing. While we could add logic in the DAG’s python file to skip execution based on Airflow variables, this wouldn’t hide DAGs from the UI, which was not our intention.```pythonimport osfrom airflow import DAGfrom airflow.operators.python import PythonOperatorfrom airflow.utils.dates import days_agofrom airflow.models import Variableimport jsoncurrent_file_name = os.path.basename(__file__)ignore_dag_files_json = Variable.get(\"ignore_dag_files\", default='[]')ignore_dag_files = json.loads(ignore_dag_files_json)if current_file_name in ignore_dag_files:    print(f\"DAG file '{current_file_name}' is in the ignore list. Skipping DAG creation.\")    exit()    with DAG(    dag_id=\"test_dag\",    schedule=None,    start_date=days_ago(1),    catchup=False,) as dag:    ...```3. Dynamically Creating .airflowignore via the API and gitsyncThis approach involved creating a temporary backup file and using gitsync.sh to push it to dags/.airflowignore``. This seemed promising and we confirmed that the files were correctly being copied, however, it didn’t work as expected.4.  Copying backed up .airflowignore:I stored .airflowignore at /$DEPLOY_ENV/base-dags/.airflowignore.backup and copy this file at dags/.airflowignore`` using gitsync.while truedo    ...     git fetch --depth 1 --prune origin $TARGET_BRANCH    git reset --hard origin/$TARGET_BRANCH    if [ -f \"/home1/irteam/deploy/$DEPLOY_ENV/base-dags/`.airflowignore`.backup\" ]; then        cp \"/home1/irteam/deploy/$DEPLOY_ENV/base-dags/`.airflowignore`.backup\" \"$GIT_DAGS_REPO_HOME/dags/`.airflowignore`\"        echo \"$(date '+%Y-%m-%d %H:%M:%S') Restored dags/`.airflowignore` from `.airflowignore`.backup\"    else        echo \"$(date '+%Y-%m-%d %H:%M:%S') `.airflowignore`.backup file not found, skipping dags/`.airflowignore` restoration\"    fi    echo \"$(date '+%Y-%m-%d %H:%M:%S') dags repo synced.\"    sleep $WAIT_TIMEdoneAlthough logs confirmed the .airflowignore was being updated, the changes weren’t being reflected in Airflow. DAGs that should have been hidden were still visible and running. I confirmed there were no permission issues or that the file was modified by another user.3. The Root of the Problem: Airflow DAG Parsing and Kubernetes VolumeI discovered that the issue was related to how Airflow detects file changes, and how file system changes are handled on a mounted volume. Here’s a breakdown:  Airflow’s File Change Detection: Airflow’s DagFileProcessorManager uses a polling interval to monitor for updates using _file_stats which stores file metadata and uses this info to populate the _file_path_queue.  How _file_stats is Updated: file_stats is updated by prepare_file_path_queue, which checks for new files using os.path.getmtime. This is key to our problem.  os.path.getmtime and Volume Metadata: os.path.getmtime retrieves modification time metadata of the file system’s node.  Git Reset and Volume Manipulation: git reset --hard directly manipulates the local file system of the volume node. This operation involves deleting the directory and creating new files or setting up metadata on existing files.  Container File Creation and CoW: When you create a file inside a container on a mounted emptyDir volume, you’re making changes in the container’s isolated layer, which is using the Copy-on-Write (CoW) mechanism. The changes are not applied to the volume’s metadata. Even though file creation affects the mtime inside the container, the actual volume node’s mtime does not change.  Airflow’s Perspective: While the Airflow scheduler runs inside the container, the scheduler calls os.path.getmtime() against the volume’s file system, not the container’s. This means it cannot detect changes made through direct file creation inside the container.  In essence, when I copy .airflowignore in our gitsync.sh , I were making changes inside the container’s file system not the volume’s, so Airflow’s file change detection mechanism was not triggered.4. ConclusionThis experiments clearly show that git reset is the only method to correctly modify the .airflowignore file, in a way that Airflow recognizes. This means the .airflowignore file needs to be managed within the Git repository itself, and the mounted volume needs to have the .airflowignore file by git reset.This ultimately means that a single branch strategy is not viable for our case, because each pipeline needs its specific .airflowignore which has to exist in Github repository. I need to manage different versions of the .airflowignore file for each pipeline and need to be stored in Github repository.",
            "content_html": "<h3 id=\"1-introduction\">1. Introduction</h3><p>I run Airflow in a Kubernetes environment, relying on a script to pull DAG definitions from a remote Git repository. My goal was to implement domain-specific DAG deployments by dynamically managing <code>.airflowignore</code> files. I attempted to inject these <code>.airflowignore</code> files directly into the Kubernetes volume where our DAGs reside. However, this seemingly straightforward approach led to unexpected challenges, uncovering some interesting details about how Airflow interacts with mounted volumes. I explored my process and lessons learned about Airflow’s DAG parsing logic.</p><h3 id=\"2-approaches\">2. Approaches</h3><p>I started by considering a few ideas:</p><h4 id=\"1-modifying-gitsyncsh-to-exclude-and-reset-airflowignore\">1. **Modifying <code>gitsync.sh</code> to exclude and reset <code>.airflowignore</code></h4><p>This was quickly discarded. Without a remote version of the desired <code>.airflowignore</code>, I couldn’t reliably restore it via command line.</p><h4 id=\"2-using-airflow-variables-to-hide-dags\">2. Using Airflow variables to hide DAGs</h4><p>I discovered that, except for <code>.airflowignore</code>, there is no way to prevent DAG parsing. While we could add logic in the DAG’s python file to skip execution based on Airflow variables, this wouldn’t hide DAGs from the UI, which was not our intention.</p><pre><code>```pythonimport osfrom airflow import DAGfrom airflow.operators.python import PythonOperatorfrom airflow.utils.dates import days_agofrom airflow.models import Variableimport jsoncurrent_file_name = os.path.basename(__file__)ignore_dag_files_json = Variable.get(\"ignore_dag_files\", default='[]')ignore_dag_files = json.loads(ignore_dag_files_json)if current_file_name in ignore_dag_files:    print(f\"DAG file '{current_file_name}' is in the ignore list. Skipping DAG creation.\")    exit()    with DAG(    dag_id=\"test_dag\",    schedule=None,    start_date=days_ago(1),    catchup=False,) as dag:    ...```</code></pre><h4 id=\"3-dynamically-creating-airflowignore-via-the-api-and-gitsync\">3. <strong>Dynamically Creating <code>.airflowignore</code> via the API and gitsync</strong></h4><p>This approach involved creating a temporary backup file and using <code>gitsync.sh</code> to push it to <code>dags/</code>.airflowignore``. This seemed promising and we confirmed that the files were correctly being copied, however, it didn’t work as expected.</p><h4 id=\"4--copying-backed-up-airflowignore\">4.  <strong>Copying backed up <code>.airflowignore</code>:</strong></h4><p>I stored <code>.airflowignore</code> at <code>/$DEPLOY_ENV/base-dags/</code>.airflowignore<code>.backup</code> and copy this file at <code>dags/</code>.airflowignore`` using gitsync.</p><pre><code class=\"language-bash\">while truedo    ...     git fetch --depth 1 --prune origin $TARGET_BRANCH    git reset --hard origin/$TARGET_BRANCH    if [ -f \"/home1/irteam/deploy/$DEPLOY_ENV/base-dags/`.airflowignore`.backup\" ]; then        cp \"/home1/irteam/deploy/$DEPLOY_ENV/base-dags/`.airflowignore`.backup\" \"$GIT_DAGS_REPO_HOME/dags/`.airflowignore`\"        echo \"$(date '+%Y-%m-%d %H:%M:%S') Restored dags/`.airflowignore` from `.airflowignore`.backup\"    else        echo \"$(date '+%Y-%m-%d %H:%M:%S') `.airflowignore`.backup file not found, skipping dags/`.airflowignore` restoration\"    fi    echo \"$(date '+%Y-%m-%d %H:%M:%S') dags repo synced.\"    sleep $WAIT_TIMEdone</code></pre><p>Although logs confirmed the <code>.airflowignore</code> was being updated, the changes weren’t being reflected in Airflow. DAGs that should have been hidden were still visible and running. I confirmed there were no permission issues or that the file was modified by another user.</p><h3 id=\"3-the-root-of-the-problem-airflow-dag-parsing-and-kubernetes-volume\">3. The Root of the Problem: Airflow DAG Parsing and Kubernetes Volume</h3><p>I discovered that the issue was related to how Airflow detects file changes, and how file system changes are handled on a mounted volume. Here’s a breakdown:</p><ul>  <li>Airflow’s File Change Detection: Airflow’s <code>DagFileProcessorManager</code> uses a polling interval to monitor for updates using <code>_file_stats</code> which stores file metadata and uses this info to populate the <code>_file_path_queue</code>.</li>  <li>How <code>_file_stats</code> is Updated: <code>file_stats</code> is updated by <code>prepare_file_path_queue</code>, which checks for new files using <code>os.path.getmtime</code>. This is key to our problem.</li>  <li><code>os.path.getmtime</code> and Volume Metadata: <code>os.path.getmtime</code> retrieves modification time metadata of the file system’s node.</li>  <li>Git Reset and Volume Manipulation: <code>git reset --hard</code> directly manipulates the local file system of the volume node. This operation involves deleting the directory and creating new files or setting up metadata on existing files.</li>  <li>Container File Creation and CoW: When you create a file inside a container on a mounted emptyDir volume, you’re making changes in the container’s isolated layer, which is using the Copy-on-Write (CoW) mechanism. The changes are not applied to the volume’s metadata. Even though file creation affects the mtime inside the container, the actual volume node’s mtime does not change.</li>  <li>Airflow’s Perspective: While the Airflow scheduler runs inside the container, the scheduler calls <code>os.path.getmtime()</code> against the volume’s file system, not the container’s. This means it cannot detect changes made through direct file creation inside the container.</li>  <li>In essence, when I copy <code>.airflowignore</code> in our <code>gitsync.sh</code> , I were making changes inside the container’s file system not the volume’s, so Airflow’s file change detection mechanism was not triggered.</li></ul><h3 id=\"4-conclusion\">4. Conclusion</h3><p>This experiments clearly show that <code>git reset</code> is the only method to correctly modify the <code>.airflowignore</code> file, in a way that Airflow recognizes. This means the <code>.airflowignore</code> file needs to be managed within the Git repository itself, and the mounted volume needs to have the <code>.airflowignore</code> file by git reset.This ultimately means that a single branch strategy is not viable for our case, because each pipeline needs its specific <code>.airflowignore</code> which has to exist in Github repository. I need to manage different versions of the <code>.airflowignore</code> file for each pipeline and need to be stored in Github repository.</p>",
            "url": "http://localhost:4000/2025/01/23/airflow-dag-parsing-how-volumes-and-file-changes-interact",
            
            
            
            "tags": ["airflow","dag","data engineering","container","docker","orchestration","DevOps"],
            
            "date_published": "2025-01-23T00:00:00+09:00",
            "date_modified": "2025-01-23T00:00:00+09:00",
            
                "author": 
                "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2025/01/19/building-rdb-from-scratch-01-b-tree",
            "title": "Building RDB from scratch - 01. B+ Tree",
            "summary": "Exploring the importance of B+ trees in building a tiny relational database (RDB) and reflecting on the implementation insights gained from examining an open-source B+ tree library.",
            "content_text": "1. Why B+ tree?When starting the construction of a lightweight relational database, the B+ tree emerges as a fundamental data structure. Its inherent ability to efficiently index and retrieve data renders it indispensable for managing structured information. Unlike traditional binary search trees, the B+ tree is specifically optimized for disk-based storage, minimizing the number of I/O operations required to access data—a crucial consideration for any relational database where data typically resides on disk, not solely in memory. Thus, the B+ tree is not merely a beneficial option, but rather a cornerstone for any viable small-scale RDB.Furthermore, examining the implementation details of a B+ tree, such as those found in the open-source collinglass/bptree library, offers valuable insights into practical database design and implementation, which I will explore in this post.2. Implementations1) Core Data Structurestype Tree struct {\tRoot *Node}type Record struct {\tValue []byte}type Node struct {\tPointers []interface{}\tKeys     []int\tParent   *Node\tIsLeaf   bool\tNumKeys  int\tNext     *Node}Tree represents the entire B+ tree structure, with the Root field serving as the entry point to the tree. It is a basic and clear way to represent the whole B+ tree structure.Record struct separates keys from their byte array values ([]byte), allowing the library to handle diverse data types as long as they can be serialized.Node represents a single B+ tree node, which holds Pointers (to child nodes or record values), Keys, a Parent pointer, a flag for IsLeaf, NumKeys tracking the number of valid keys, and a Next pointer for linked list of leaf nodes. The interface{} type for Pointers enables handling both nodes and records, and NumKeys ensures performance optimization to determine the number of valid keys.2) Insertionfunc (t *Tree) Insert(key int, value []byte) error {\t// ...\tif t.Root == nil {\t\treturn t.startNewTree(key, pointer)\t}\t// ...    leaf = t.findLeaf(key, false)    \tif leaf.NumKeys &lt; order-1 {\t\tinsertIntoLeaf(leaf, key, pointer)\t\treturn nil\t}\treturn t.insertIntoLeafAfterSplitting(leaf, key, pointer)}The insertion process first checks for a root node, creating a new tree if necessary. It locates the correct leaf node via t.findLeaf and then inserts the key if there’s capacity, using insertIntoLeaf as a helper method. When the leaf is full, t.insertIntoLeafAfterSplitting handles the node split, utilizing temporary arrays and demonstrating a “copy-on-write” approach where a new node is created during splits to keep the B+ tree balanced.3) Searchingfunc (t *Tree) Find(key int, verbose bool) (*Record, error) {\ti := 0\tc := t.findLeaf(key, verbose)\tif c == nil {\t\treturn nil, errors.New(\"key not found\")\t}\tfor i = 0; i &lt; c.NumKeys; i++ {\t\tif c.Keys[i] == key {\t\t\tbreak\t\t}\t}\tif i == c.NumKeys {\t\treturn nil, errors.New(\"key not found\")\t}\tr, _ := c.Pointers[i].(*Record)\treturn r, nil}The search starts with the private findLeaf function, which recursively navigates the tree to locate the appropriate leaf node. Within the leaf node, a loop iterates through the keys to find a match. Upon finding a matching key, the associated value is returned after a type assertion to the Record type. This search logic demonstrates how each node acts as a decision point, guiding the traversal along the correct search path to find the target key.4) Deletionfunc (t *Tree) Delete(key int) error {\tkey_record, err := t.Find(key, false)\tif err != nil {\t\treturn err\t}\tkey_leaf := t.findLeaf(key, false)\tif key_record != nil &amp;&amp; key_leaf != nil {\t\tt.deleteEntry(key_leaf, key, key_record)\t}\treturn nil}The deletion process begins by using t.Find and t.findLeaf to locate the target node and key. Once found, the t.deleteEntry method is invoked, which orchestrates a complex deletion procedure. This involves removeEntryFromNode to remove the key from the node, and then a series of other operations: adjustRoot to handle root node modifications, and either coalesceNodes or redistributeNodes to maintain the B+ tree’s balance through merging or redistributing nodes, respectively.",
            "content_html": "<h3 id=\"1-why-b-tree\">1. Why B+ tree?</h3><p>When starting the construction of a lightweight relational database, the B+ tree emerges as a fundamental data structure. Its inherent ability to efficiently index and retrieve data renders it indispensable for managing structured information. Unlike traditional binary search trees, the B+ tree is specifically optimized for disk-based storage, minimizing the number of I/O operations required to access data—a crucial consideration for any relational database where data typically resides on disk, not solely in memory. Thus, the B+ tree is not merely a beneficial option, but rather a cornerstone for any viable small-scale RDB.<br />Furthermore, examining the implementation details of a B+ tree, such as those found in the open-source <code>collinglass/bptree</code> library, offers valuable insights into practical database design and implementation, which I will explore in this post.</p><h3 id=\"2-implementations\">2. Implementations</h3><h4 id=\"1-core-data-structures\">1) Core Data Structures</h4><pre><code class=\"language-gotemplate\">type Tree struct {\tRoot *Node}type Record struct {\tValue []byte}type Node struct {\tPointers []interface{}\tKeys     []int\tParent   *Node\tIsLeaf   bool\tNumKeys  int\tNext     *Node}</code></pre><p>Tree represents the entire B+ tree structure, with the Root field serving as the entry point to the tree. It is a basic and clear way to represent the whole B+ tree structure.<br />Record struct separates keys from their byte array values ([]byte), allowing the library to handle diverse data types as long as they can be serialized.<br />Node represents a single B+ tree node, which holds Pointers (to child nodes or record values), Keys, a Parent pointer, a flag for IsLeaf, NumKeys tracking the number of valid keys, and a Next pointer for linked list of leaf nodes. The interface{} type for Pointers enables handling both nodes and records, and NumKeys ensures performance optimization to determine the number of valid keys.</p><h4 id=\"2-insertion\">2) Insertion</h4><pre><code class=\"language-gotemplate\">func (t *Tree) Insert(key int, value []byte) error {\t// ...\tif t.Root == nil {\t\treturn t.startNewTree(key, pointer)\t}\t// ...    leaf = t.findLeaf(key, false)    \tif leaf.NumKeys &lt; order-1 {\t\tinsertIntoLeaf(leaf, key, pointer)\t\treturn nil\t}\treturn t.insertIntoLeafAfterSplitting(leaf, key, pointer)}</code></pre><p>The insertion process first checks for a root node, creating a new tree if necessary. It locates the correct leaf node via t.findLeaf and then inserts the key if there’s capacity, using insertIntoLeaf as a helper method. When the leaf is full, t.insertIntoLeafAfterSplitting handles the node split, utilizing temporary arrays and demonstrating a “copy-on-write” approach where a new node is created during splits to keep the B+ tree balanced.</p><h4 id=\"3-searching\">3) Searching</h4><pre><code class=\"language-gotemplate\">func (t *Tree) Find(key int, verbose bool) (*Record, error) {\ti := 0\tc := t.findLeaf(key, verbose)\tif c == nil {\t\treturn nil, errors.New(\"key not found\")\t}\tfor i = 0; i &lt; c.NumKeys; i++ {\t\tif c.Keys[i] == key {\t\t\tbreak\t\t}\t}\tif i == c.NumKeys {\t\treturn nil, errors.New(\"key not found\")\t}\tr, _ := c.Pointers[i].(*Record)\treturn r, nil}</code></pre><p>The search starts with the private findLeaf function, which recursively navigates the tree to locate the appropriate leaf node. Within the leaf node, a loop iterates through the keys to find a match. Upon finding a matching key, the associated value is returned after a type assertion to the Record type. This search logic demonstrates how each node acts as a decision point, guiding the traversal along the correct search path to find the target key.</p><h4 id=\"4-deletion\">4) Deletion</h4><pre><code class=\"language-gotemplate\">func (t *Tree) Delete(key int) error {\tkey_record, err := t.Find(key, false)\tif err != nil {\t\treturn err\t}\tkey_leaf := t.findLeaf(key, false)\tif key_record != nil &amp;&amp; key_leaf != nil {\t\tt.deleteEntry(key_leaf, key, key_record)\t}\treturn nil}</code></pre><p>The deletion process begins by using t.Find and t.findLeaf to locate the target node and key. Once found, the t.deleteEntry method is invoked, which orchestrates a complex deletion procedure. This involves removeEntryFromNode to remove the key from the node, and then a series of other operations: adjustRoot to handle root node modifications, and either coalesceNodes or redistributeNodes to maintain the B+ tree’s balance through merging or redistributing nodes, respectively.</p>",
            "url": "http://localhost:4000/2025/01/19/building-rdb-from-scratch-01-b-tree",
            
            
            
            "tags": ["database","RDB","B+ tree","data structures","algorithms","implementation","software engineering"],
            
            "date_published": "2025-01-19T00:00:00+09:00",
            "date_modified": "2025-01-19T00:00:00+09:00",
            
                "author": 
                "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2025/01/14/gpt-1-2-3-and-4",
            "title": "GPT 1, 2, 3 and 4",
            "summary": "Explore the evolution of Generative Pre-trained Transformers (GPT) through an in-depth analysis of the key advancements from GPT-1 to GPT-4. This post highlights the transformative techniques and capabilities that have shaped natural language processing.",
            "content_text": "1) GPT-1GPT-1 introduced a groundbreaking approach to natural language processing by leveraging the power of unsupervised pre-training. This technique involves training a language model on a massive amount of text data without any explicit labels. The resulting model, equipped with a deep understanding of language, can then be fine-tuned on specific tasks with relatively small labeled datasets.Key Differences from Traditional Supervised Learning  Pre-training: GPT-1’s approach starts with a pre-trained model, whereas traditional supervised learning trains a model from random initialization.  Data Efficiency: GPT-1 requires significantly less labeled data for fine-tuning, making it more practical for real-world applications.  Transfer Learning: GPT-1 enables transfer learning, allowing the model to adapt to new tasks with minimal effort.How does supervised fine-tuning work in GPT-1?  Pre-trained Model: A language model is first trained on a massive corpus of text data. This pre-training step allows the model to learn the statistical properties of language, such as grammar and semantics.  Task-Specific Fine-tuning: The pre-trained model is then adapted to a specific task using a labeled dataset. This involves adding a linear layer to the model and training it on the new task while keeping most of the pre-trained parameters fixed.  Input Transformations: To accommodate various tasks, GPT-1 employs input transformations. For instance, in question answering, the question, context, and answer choices are concatenated into a single sequence.  Objective Function: The model is trained to minimize a combined loss function that includes both a supervised loss for the specific task and an unsupervised loss to preserve the language modeling capabilities.2) GPT-2Key Advancements of GPT-2GPT-1 introduced the concept of pre-training a language model on a massive amount of text data and then fine-tuning it on specific tasks.GPT-2 took this concept to the next level by training an even larger model on a more diverse dataset.  Increased Model Size: GPT-2 employed significantly larger models, allowing for more complex pattern recognition and better generalization.  Unsupervised Multitask Learning: Unlike GPT-1, which relied on supervised fine-tuning for specific tasks, GPT-2 demonstrated the ability to perform a wide range of tasks directly from the pre-trained model, without explicit task-specific training.  Improved Zero-Shot Learning: GPT-2 exhibited remarkable zero-shot learning capabilities, meaning it could perform tasks it had not been explicitly trained on, simply by providing the task as text.3) GPT-3GPT-3 represents a significant leap forward in the evolution of large language models, building upon the successes of its predecessors, GPT-1 and GPT-2.Key Advancements of GPT-3  Massive Scale: GPT-3 boasts a significantly larger number of parameters compared to GPT-2, resulting in a dramatic increase in model capacity and computational power.  Enhanced Capabilities: GPT-3 demonstrated impressive abilities in various NLP tasks, including:          Text generation: Generating human-like text, writing stories, translating languages, and even composing poems.      Question answering: Providing comprehensive and informative answers to a wide range of questions.      Code generation: Writing and debugging code in various programming languages.      Creative content creation: Generating novel content, such as scripts, articles, and even musical pieces.        In-context learning: GPT-3 exhibited remarkable in-context learning capabilities, demonstrating the ability to adapt to new tasks or concepts simply by providing a few examples within the input.  Few-shot learning: GPT-3 achieved impressive results in few-shot learning scenarios, where the model is provided with only a few labeled examples for a new task.4) GPT-4GPT-4 represents the latest advancement in the GPT series of large language models, pushing the boundaries of AI capabilities even further.Key Advancements of GPT-4  Enhanced Capabilities: GPT-4 demonstrates significantly improved performance across a wide range of tasks, including:          Creativity: Generating more creative and innovative text formats, such as poems, code, and scripts.      Problem-solving: Exhibiting enhanced reasoning and problem-solving abilities, including the ability to handle more complex and nuanced tasks.      Safety and Reliability: GPT-4 has been developed with a strong emphasis on safety and reliability, aiming to minimize biases, hallucinations, and harmful outputs.        Multimodality: GPT-4 introduces multimodality, allowing it to accept and process both text and image inputs. This opens up new possibilities for applications such as image captioning, visual question answering, and more.  Advanced In-context Learning: GPT-4 further enhances in-context learning capabilities, allowing it to learn and adapt to new tasks with even fewer examples.",
            "content_html": "<h3 id=\"1-gpt-1\">1) GPT-1</h3><p>GPT-1 introduced a groundbreaking approach to natural language processing by leveraging the power of unsupervised pre-training. This technique involves training a language model on a massive amount of text data without any explicit labels. The resulting model, equipped with a deep understanding of language, can then be fine-tuned on specific tasks with relatively small labeled datasets.</p><h4 id=\"key-differences-from-traditional-supervised-learning\">Key Differences from Traditional Supervised Learning</h4><ul>  <li>Pre-training: GPT-1’s approach starts with a pre-trained model, whereas traditional supervised learning trains a model from random initialization.</li>  <li>Data Efficiency: GPT-1 requires significantly less labeled data for fine-tuning, making it more practical for real-world applications.</li>  <li>Transfer Learning: GPT-1 enables transfer learning, allowing the model to adapt to new tasks with minimal effort.</li></ul><h4 id=\"how-does-supervised-fine-tuning-work-in-gpt-1\">How does supervised fine-tuning work in GPT-1?</h4><ul>  <li>Pre-trained Model: A language model is first trained on a massive corpus of text data. This pre-training step allows the model to learn the statistical properties of language, such as grammar and semantics.</li>  <li>Task-Specific Fine-tuning: The pre-trained model is then adapted to a specific task using a labeled dataset. This involves adding a linear layer to the model and training it on the new task while keeping most of the pre-trained parameters fixed.</li>  <li>Input Transformations: To accommodate various tasks, GPT-1 employs input transformations. For instance, in question answering, the question, context, and answer choices are concatenated into a single sequence.</li>  <li>Objective Function: The model is trained to minimize a combined loss function that includes both a supervised loss for the specific task and an unsupervised loss to preserve the language modeling capabilities.</li></ul><p><br /></p><h3 id=\"2-gpt-2\">2) GPT-2</h3><h4 id=\"key-advancements-of-gpt-2\">Key Advancements of GPT-2</h4><p>GPT-1 introduced the concept of pre-training a language model on a massive amount of text data and then fine-tuning it on specific tasks.GPT-2 took this concept to the next level by training an even larger model on a more diverse dataset.</p><ul>  <li>Increased Model Size: GPT-2 employed significantly larger models, allowing for more complex pattern recognition and better generalization.</li>  <li>Unsupervised Multitask Learning: Unlike GPT-1, which relied on supervised fine-tuning for specific tasks, GPT-2 demonstrated the ability to perform a wide range of tasks directly from the pre-trained model, without explicit task-specific training.</li>  <li>Improved Zero-Shot Learning: GPT-2 exhibited remarkable zero-shot learning capabilities, meaning it could perform tasks it had not been explicitly trained on, simply by providing the task as text.</li></ul><p><br /></p><h3 id=\"3-gpt-3\">3) GPT-3</h3><p>GPT-3 represents a significant leap forward in the evolution of large language models, building upon the successes of its predecessors, GPT-1 and GPT-2.</p><h4 id=\"key-advancements-of-gpt-3\">Key Advancements of GPT-3</h4><ul>  <li>Massive Scale: GPT-3 boasts a significantly larger number of parameters compared to GPT-2, resulting in a dramatic increase in model capacity and computational power.</li>  <li>Enhanced Capabilities: GPT-3 demonstrated impressive abilities in various NLP tasks, including:    <ul>      <li>Text generation: Generating human-like text, writing stories, translating languages, and even composing poems.</li>      <li>Question answering: Providing comprehensive and informative answers to a wide range of questions.</li>      <li>Code generation: Writing and debugging code in various programming languages.</li>      <li>Creative content creation: Generating novel content, such as scripts, articles, and even musical pieces.</li>    </ul>  </li>  <li>In-context learning: GPT-3 exhibited remarkable in-context learning capabilities, demonstrating the ability to adapt to new tasks or concepts simply by providing a few examples within the input.</li>  <li>Few-shot learning: GPT-3 achieved impressive results in few-shot learning scenarios, where the model is provided with only a few labeled examples for a new task.</li></ul><p><br /></p><h3 id=\"4-gpt-4\">4) GPT-4</h3><p>GPT-4 represents the latest advancement in the GPT series of large language models, pushing the boundaries of AI capabilities even further.</p><h4 id=\"key-advancements-of-gpt-4\">Key Advancements of GPT-4</h4><ul>  <li>Enhanced Capabilities: GPT-4 demonstrates significantly improved performance across a wide range of tasks, including:    <ul>      <li>Creativity: Generating more creative and innovative text formats, such as poems, code, and scripts.</li>      <li>Problem-solving: Exhibiting enhanced reasoning and problem-solving abilities, including the ability to handle more complex and nuanced tasks.</li>      <li>Safety and Reliability: GPT-4 has been developed with a strong emphasis on safety and reliability, aiming to minimize biases, hallucinations, and harmful outputs.</li>    </ul>  </li>  <li>Multimodality: GPT-4 introduces multimodality, allowing it to accept and process both text and image inputs. This opens up new possibilities for applications such as image captioning, visual question answering, and more.</li>  <li>Advanced In-context Learning: GPT-4 further enhances in-context learning capabilities, allowing it to learn and adapt to new tasks with even fewer examples.</li></ul>",
            "url": "http://localhost:4000/2025/01/14/gpt-1-2-3-and-4",
            
            
            
            "tags": ["GPT","artificial intelligence","natural language processing","NLP","machine learning","deep learning","transformers"],
            
            "date_published": "2025-01-14T00:00:00+09:00",
            "date_modified": "2025-01-14T00:00:00+09:00",
            
                "author": 
                "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2025/01/02/computational-limits-and-simulation-argument",
            "title": "Computational Limits and Simulation Argument",
            "summary": "This blog post examines the computational requirements for simulating human consciousness and human history, drawing insights from research papers on the simulation argument. It highlights the immense computational demands of such simulations and explores the limitations of current technologies. The post also discusses promising avenues for future advancements, such as quantum computing and specialized hardware, that could potentially bring us closer to the realm of sophisticated simulations.",
            "content_text": "1) Computational Demands of Simulating RealityThe simulation hypothesis, a thought-provoking concept, posits that we might be living within a sophisticated computer simulation. A key question arises: could such a feature even be technically possible?The sheer computational power required to emulate a human mind is mind-boggling. Researchers have proposed estimates ranging from a modest 10^14 operations per second, based on replicating a simple neural function like contrast enhancement in the retina, to a staggering 10^16-10^17 operations per second, considering the sheer number of synapses firing in the human brain. These figures, however, may be conservative. Simulating the intricate details of synaptic interactions and dendritic trees could demand even more processing power.Fortunately, there’s reason for optimism. The human brain likely employs redundancy at the microscopic level to compensate for the inherent unreliability of individual neurons. This suggests that more efficient, non-biological processors could achieve similar results with significantly less computational overhead.  The amount of computing power needed to emulate a human mind can likewise be roughly estimated. One estimate, based on how computationally expensive it is to replicate the functionality of a piece of nervous tissue that we have already understood and whose functionality has been replicated in silico, contrast enhancement in the retina, yields a figure of ~10^14 operations per second for the entire human brain. An alternative estimate, based the number of synapses in the brain and their firing frequency, gives a figure of ~10^16-10^17 operations per second. Conceivably, even more could be required if we want to simulate in detail the internal workings of synapses and dendritic trees. However, it is likely that the human central nervous system has a high degree of redundancy on the mircoscale to compensate for the unreliability and noisiness of its neuronal components. One would therefore expect a substantial efficiency gain when using more reliable and versatile non-biological processors.  Bostrom, Nick. “Are You Living in a Computer Simulation?” Philosophical Quarterly, 53.211 (2003): 243-255.The true challenge lies in simulating not just individual minds, but the entire tapestry of human history – a task of unimaginable scale. Researchers estimate that simulating human history with sufficient fidelity could require a staggering 10^33 to 10^36 operations. While this seems astronomical, the potential for such a feat may lie in the realm of advanced, future technologies.The concept of a “planetary-mass computer,” a hypothetical machine utilizing the entire mass of a planet for computational purposes, offers a glimpse into the potential scale of future computing power. Even with conservative estimates of nanotechnological capabilities, such a computer could theoretically simulate the entire history of humankind with a minuscule fraction of its processing power.  It thus seems plausible that the main computational cost in creating simulations that are indistinguishable from physical reality for human minds in the simulation resides in simulating organic brains down to the neuronal or sub-neuronal level. While it is not possible to get a very exact estimate of the cost of a realistic simulation of human history, we can use ~10^33 - 10^36 operations as a rough estimate. As we gain more experience with virtual reality, we will get a better grasp of the computational requirements for making such worlds appear realistic to their visitors. But in any case, even if our estimate is off by several orders of magnitude, this does not matter much for our argument. We noted that a rough approximation of the computational power of a planetary-mass computer is 10^42 operations per second, and that assumes only already known nanotechnological designs, which are probably far from optimal. A single such a computer could simulate the entire mental history of humankind (call this an ancestor-simulation) by using less than one millionth of its processing power for one second. A posthuman civilization may eventually build an astronomical number of such computers. We can conclude that the computing power available to a posthuman civilization is sufficient to run a huge number of ancestor-simulations even it allocates only a minute fraction of its resources to that purpose. We can draw this conclusion even while leaving a substantial margin of error in all our estimates.  Bostrom, N. (2003). Are you living in a computer simulation?. Philosophical Quarterly, 53(211), 243-255.I was inspired by Nick Bostrom’s exploration of the computational requirements for simulating a reality like ours. By delving into the staggering figures he presents, we sought to answer a fundamental question: Where do we stand in terms of our computational capabilities, and what are the potential limits to what we can achieve?2) Where are we?SupercomputingThe quest to simulate complex systems like the human brain or entire universes demands immense computational power. While we’re still far from achieving the scale necessary for such grand simulations, advancements in supercomputing are steadily narrowing the gap.One notable example is the Aurora supercomputer at Argonne National Laboratory. This machine has achieved a sustained performance of 1.012 exaFLOPS, meaning it can perform over a quadrillion floating-point operations per second. To put this into perspective, that’s equivalent to roughly 10^18 FLOPS. The theoretical peak performance of Aurora is even more impressive, reaching nearly 2×10^18 FLOPS.GPUWhile supercomputers provide immense raw computing power, GPUs have emerged as a dominant force in accelerating specific tasks, particularly in the realm of artificial intelligence. NVIDIA’s latest RTX 50 series showcases the remarkable strides made in GPU technology. These GPUs are capable of performing 4000 AI TOPS (trillions of operations per second), representing a substantial leap over previous generations.Key features of the RTX 50 series include:  Blackwell architecture: This cutting-edge architecture offers significant improvements in performance and power efficiency.  Power consumption: The RTX 5090 model delivers exceptional performance while consuming around 600 watts of power.  Memory bandwidth: Utilizing GDDR7 memory, these GPUs boast a massive 1.8 TB/s of memory bandwidth.ASICsBeyond general-purpose processors and GPUs, specialized chips known as Application-Specific Integrated Circuits (ASICs) are emerging as crucial players in accelerating simulations. Tailored to specific tasks, ASICs offer unparalleled performance and energy efficiency. AI accelerators, a prominent example, are designed to optimize deep learning computations. Recent advancements in AI accelerators have achieved remarkable milestones, with some chips exceeding 100 TeraFLOPS of AI performance. These chips are driving breakthroughs in natural language processing, computer vision, and other AI-driven applications.Furthermore, the rise of neuromorphic chips, inspired by the human brain’s neural architecture, is opening new avenues for simulation. These chips, with their inherent parallelism and low-power operation, are particularly well-suited for simulating biological systems and developing more energy-efficient AI algorithms. While still in their early stages, neuromorphic chips hold immense promise for revolutionizing fields such as neuroscience, robotics, and AI.3) LimitationsWhile the computational power demonstrated by supercomputers, GPUs, and specialized ASICs is impressive, it’s essential to acknowledge the significant limitations that still exist. The vast computational resources required to simulate complex systems like the human brain or the universe are far beyond our current capabilities.The Insufficiency of Current EstimatesThe figures presented in “Where are we?”, while staggering, merely scratch the surface of the computational demands involved in creating truly comprehensive simulations. These estimates are often based on simplified models of neural networks or physical systems, and they may not accurately reflect the complexity of real-world phenomena. For instance, the human brain is a highly dynamic and interconnected system that is not fully understood, making it difficult to precisely quantify the computational requirements for simulating its functions.Fundamental Limits of ComputationEven if we could overcome the engineering challenges of building more powerful computers, there are fundamental limits to computation that may constrain our ability to simulate complex systems. These limits include:  The physical limits of computing: As we continue to miniaturize transistors, we eventually reach the quantum realm, where quantum effects can disrupt computations.  The heat dissipation problem: As computers become more powerful, they generate more heat, which can limit performance and lead to system failures.  The complexity of the universe: The universe may be inherently chaotic and unpredictable, making it impossible to create a perfect simulation.  Algorithmic limitations: Even with infinite computational power, we may not have the algorithms necessary to simulate certain phenomena.",
            "content_html": "<h3 id=\"1-computational-demands-of-simulating-reality\">1) Computational Demands of Simulating Reality</h3><p>The simulation hypothesis, a thought-provoking concept, posits that we might be living within a sophisticated computer simulation. A key question arises: could such a feature even be technically possible?<br />The sheer computational power required to emulate a human mind is mind-boggling. Researchers have proposed estimates ranging from a modest 10^14 operations per second, based on replicating a simple neural function like contrast enhancement in the retina, to a staggering 10^16-10^17 operations per second, considering the sheer number of synapses firing in the human brain. These figures, however, may be conservative. Simulating the intricate details of synaptic interactions and dendritic trees could demand even more processing power.Fortunately, there’s reason for optimism. The human brain likely employs redundancy at the microscopic level to compensate for the inherent unreliability of individual neurons. This suggests that more efficient, non-biological processors could achieve similar results with significantly less computational overhead.</p><blockquote>  <p>The amount of computing power needed to emulate a human mind can likewise be roughly estimated. One estimate, based on how computationally expensive it is to replicate the functionality of a piece of nervous tissue that we have already understood and whose functionality has been replicated in silico, contrast enhancement in the retina, yields a figure of ~10^14 operations per second for the entire human brain. An alternative estimate, based the number of synapses in the brain and their firing frequency, gives a figure of ~10^16-10^17 operations per second. Conceivably, even more could be required if we want to simulate in detail the internal workings of synapses and dendritic trees. However, it is likely that the human central nervous system has a high degree of redundancy on the mircoscale to compensate for the unreliability and noisiness of its neuronal components. One would therefore expect a substantial efficiency gain when using more reliable and versatile non-biological processors.</p>  <p>Bostrom, Nick. “Are You Living in a Computer Simulation?” Philosophical Quarterly, 53.211 (2003): 243-255.</p></blockquote><p>The true challenge lies in simulating not just individual minds, but the entire tapestry of human history – a task of unimaginable scale. Researchers estimate that simulating human history with sufficient fidelity could require a staggering 10^33 to 10^36 operations. While this seems astronomical, the potential for such a feat may lie in the realm of advanced, future technologies.The concept of a “planetary-mass computer,” a hypothetical machine utilizing the entire mass of a planet for computational purposes, offers a glimpse into the potential scale of future computing power. Even with conservative estimates of nanotechnological capabilities, such a computer could theoretically simulate the entire history of humankind with a minuscule fraction of its processing power.</p><blockquote>  <p>It thus seems plausible that the main computational cost in creating simulations that are indistinguishable from physical reality for human minds in the simulation resides in simulating organic brains down to the neuronal or sub-neuronal level. While it is not possible to get a very exact estimate of the cost of a realistic simulation of human history, we can use ~10^33 - 10^36 operations as a rough estimate. As we gain more experience with virtual reality, we will get a better grasp of the computational requirements for making such worlds appear realistic to their visitors. But in any case, even if our estimate is off by several orders of magnitude, this does not matter much for our argument. We noted that a rough approximation of the computational power of a planetary-mass computer is 10^42 operations per second, and that assumes only already known nanotechnological designs, which are probably far from optimal. A single such a computer could simulate the entire mental history of humankind (call this an ancestor-simulation) by using less than one millionth of its processing power for one second. A posthuman civilization may eventually build an astronomical number of such computers. We can conclude that the computing power available to a posthuman civilization is sufficient to run a huge number of ancestor-simulations even it allocates only a minute fraction of its resources to that purpose. We can draw this conclusion even while leaving a substantial margin of error in all our estimates.</p>  <p>Bostrom, N. (2003). Are you living in a computer simulation?. Philosophical Quarterly, 53(211), 243-255.</p></blockquote><p>I was inspired by Nick Bostrom’s exploration of the computational requirements for simulating a reality like ours. By delving into the staggering figures he presents, we sought to answer a fundamental question: Where do we stand in terms of our computational capabilities, and what are the potential limits to what we can achieve?</p><h3 id=\"2-where-are-we\">2) Where are we?</h3><h4 id=\"supercomputing\">Supercomputing</h4><p>The quest to simulate complex systems like the human brain or entire universes demands immense computational power. While we’re still far from achieving the scale necessary for such grand simulations, advancements in supercomputing are steadily narrowing the gap.</p><p>One notable example is the Aurora supercomputer at Argonne National Laboratory. This machine has achieved a sustained performance of 1.012 exaFLOPS, meaning it can perform over a quadrillion floating-point operations per second. To put this into perspective, that’s equivalent to roughly 10^18 FLOPS. The theoretical peak performance of Aurora is even more impressive, reaching nearly 2×10^18 FLOPS.</p><h4 id=\"gpu\">GPU</h4><p>While supercomputers provide immense raw computing power, GPUs have emerged as a dominant force in accelerating specific tasks, particularly in the realm of artificial intelligence. NVIDIA’s latest RTX 50 series showcases the remarkable strides made in GPU technology. These GPUs are capable of performing 4000 AI TOPS (trillions of operations per second), representing a substantial leap over previous generations.</p><p>Key features of the RTX 50 series include:</p><ul>  <li>Blackwell architecture: This cutting-edge architecture offers significant improvements in performance and power efficiency.</li>  <li>Power consumption: The RTX 5090 model delivers exceptional performance while consuming around 600 watts of power.</li>  <li>Memory bandwidth: Utilizing GDDR7 memory, these GPUs boast a massive 1.8 TB/s of memory bandwidth.</li></ul><h4 id=\"asics\">ASICs</h4><p>Beyond general-purpose processors and GPUs, specialized chips known as Application-Specific Integrated Circuits (ASICs) are emerging as crucial players in accelerating simulations. Tailored to specific tasks, ASICs offer unparalleled performance and energy efficiency. AI accelerators, a prominent example, are designed to optimize deep learning computations. Recent advancements in AI accelerators have achieved remarkable milestones, with some chips exceeding 100 TeraFLOPS of AI performance. These chips are driving breakthroughs in natural language processing, computer vision, and other AI-driven applications.</p><p>Furthermore, the rise of neuromorphic chips, inspired by the human brain’s neural architecture, is opening new avenues for simulation. These chips, with their inherent parallelism and low-power operation, are particularly well-suited for simulating biological systems and developing more energy-efficient AI algorithms. While still in their early stages, neuromorphic chips hold immense promise for revolutionizing fields such as neuroscience, robotics, and AI.</p><h3 id=\"3-limitations\">3) Limitations</h3><p>While the computational power demonstrated by supercomputers, GPUs, and specialized ASICs is impressive, it’s essential to acknowledge the significant limitations that still exist. The vast computational resources required to simulate complex systems like the human brain or the universe are far beyond our current capabilities.</p><h4 id=\"the-insufficiency-of-current-estimates\">The Insufficiency of Current Estimates</h4><p>The figures presented in “Where are we?”, while staggering, merely scratch the surface of the computational demands involved in creating truly comprehensive simulations. These estimates are often based on simplified models of neural networks or physical systems, and they may not accurately reflect the complexity of real-world phenomena. For instance, the human brain is a highly dynamic and interconnected system that is not fully understood, making it difficult to precisely quantify the computational requirements for simulating its functions.</p><h4 id=\"fundamental-limits-of-computation\">Fundamental Limits of Computation</h4><p>Even if we could overcome the engineering challenges of building more powerful computers, there are fundamental limits to computation that may constrain our ability to simulate complex systems. These limits include:</p><ul>  <li>The physical limits of computing: As we continue to miniaturize transistors, we eventually reach the quantum realm, where quantum effects can disrupt computations.</li>  <li>The heat dissipation problem: As computers become more powerful, they generate more heat, which can limit performance and lead to system failures.</li>  <li>The complexity of the universe: The universe may be inherently chaotic and unpredictable, making it impossible to create a perfect simulation.</li>  <li>Algorithmic limitations: Even with infinite computational power, we may not have the algorithms necessary to simulate certain phenomena.</li></ul>",
            "url": "http://localhost:4000/2025/01/02/computational-limits-and-simulation-argument",
            
            
            
            "tags": ["simulation","computational limits","simulation argument","computer science","artificial intelligence","quantum computing","philosophy"],
            
            "date_published": "2025-01-02T00:00:00+09:00",
            "date_modified": "2025-01-02T00:00:00+09:00",
            
                "author": 
                "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/12/20/on-the-impossibility-of-change",
            "title": "On the Impossibility of Change",
            "summary": "Explore the profound psychological and philosophical implications of feeling trapped in a seemingly unchanging state. This post delves into the personal experiences and societal observations that highlight the human desire for flexibility and the potential consequences of feeling stuck.",
            "content_text": "ReminiscenceI’ve often felt most alive when possibilities stretched out before me like an endless horizon. College, for instance, was a time of boundless exploration. I wandered through countless academic paths, discovering countless versions of myself. But it’s the moments when I’ve felt trapped that have truly shaped my perspective.My early days as an engineer were a stark example. I was a cog in a massive machine, my future seemingly predetermined. The realization was chilling. It was as if I were trapped in a glass box, watching my life play out on a loop.Perhaps that’s why I’ve come to believe that the most miserable moments of my life have been those defined by a sense of permanence. When we feel stuck, when we believe that change is impossible, we become prisoners of our own making. I wonder, do others share this sentiment? Are we all searching for a way to escape the confines of our seemingly fixed realities?śūnyatāThe illusion of an enduring, unchanging “self,” a solidified ātman in the language of ancient Indian philosophy, is the root of much of our suffering. It’s in these moments of perceived stagnation, when the dharma of constant change seems to have ceased, that despair takes root. But the truth, as I’ve come to understand through the lens of śūnyatā, is that there is no fixed, independent self. We are a continuous flow, a dynamic interplay of skandhas – ever-shifting aggregates of form, sensation, perception, volition, and consciousness. To resist this flow, to cling to a particular state or identity, is to invite duhkha, the suffering arising from craving and clinging. It’s like trying to grasp a handful of water – the harder you squeeze, the more it slips away. True freedom lies not in resisting the inevitable flux of existence, but in embracing the anicca, the impermanence of all things.In retrospect, I’ve come to realize how profoundly the concept of śūnyatā has permeated my understanding of existence. Recognizing the impermanence inherent in all phenomena has enabled me to cultivate a greater sense of acceptance, allowing me to navigate life’s inevitable ups and downs with greater equanimity. The understanding that all things are subject to change has not only alleviated suffering but has also deepened my appreciation for the present moment.Dynamic SelfJung emphasized the process of individuation, where the self integrates with the unconscious, moving toward a more complete state. This process can be likened to a butterfly emerging from its chrysalis, transforming into a wholly new being. Once the self has changed, it cannot revert to its previous state. We are inherently changing beings, and by embracing this change and living actively within it, we can achieve true freedom of being.",
            "content_html": "<h3 id=\"reminiscence\">Reminiscence</h3><p>I’ve often felt most alive when possibilities stretched out before me like an endless horizon. College, for instance, was a time of boundless exploration. I wandered through countless academic paths, discovering countless versions of myself. But it’s the moments when I’ve felt trapped that have truly shaped my perspective.<br />My early days as an engineer were a stark example. I was a cog in a massive machine, my future seemingly predetermined. The realization was chilling. It was as if I were trapped in a glass box, watching my life play out on a loop.<br />Perhaps that’s why I’ve come to believe that the most miserable moments of my life have been those defined by a sense of permanence. When we feel stuck, when we believe that change is impossible, we become prisoners of our own making. I wonder, do others share this sentiment? Are we all searching for a way to escape the confines of our seemingly fixed realities?</p><h3 id=\"śūnyatā\">śūnyatā</h3><p>The illusion of an enduring, unchanging “self,” a solidified <a href=\"https://en.wikipedia.org/wiki/%C4%80tman_(Hinduism)\">ātman</a> in the language of ancient Indian philosophy, is the root of much of our suffering. It’s in these moments of perceived stagnation, when the dharma of constant change seems to have ceased, that despair takes root. <br />But the truth, as I’ve come to understand through the lens of <a href=\"https://en.wikipedia.org/wiki/%C5%9A%C5%ABnyat%C4%81\">śūnyatā</a>, is that there is no fixed, independent self. We are a continuous flow, a dynamic interplay of <a href=\"https://en.wikipedia.org/wiki/Skandha\">skandhas</a> – ever-shifting aggregates of form, sensation, perception, volition, and consciousness. To resist this flow, to cling to a particular state or identity, is to invite <a href=\"https://en.wikipedia.org/wiki/Du%E1%B8%A5kha\">duhkha</a>, the suffering arising from craving and clinging. It’s like trying to grasp a handful of water – the harder you squeeze, the more it slips away. True freedom lies not in resisting the inevitable flux of existence, but in embracing the anicca, the impermanence of all things.<br />In retrospect, I’ve come to realize how profoundly the concept of śūnyatā has permeated my understanding of existence. Recognizing the impermanence inherent in all phenomena has enabled me to cultivate a greater sense of acceptance, allowing me to navigate life’s inevitable ups and downs with greater equanimity. The understanding that all things are subject to change has not only alleviated suffering but has also deepened my appreciation for the present moment.</p><h3 id=\"dynamic-self\">Dynamic Self</h3><p>Jung emphasized the process of <a href=\"https://scottjeffrey.com/individuation-process/\">individuation</a>, where the self integrates with the unconscious, moving toward a more complete state. This process can be likened to a butterfly emerging from its chrysalis, transforming into a wholly new being. Once the self has changed, it cannot revert to its previous state. We are inherently changing beings, and by embracing this change and living actively within it, we can achieve true freedom of being.</p>",
            "url": "http://localhost:4000/2024/12/20/on-the-impossibility-of-change",
            
            
            
            "tags": ["psychology","philosophy","change","stagnation","personal growth","self-reflection"],
            
            "date_published": "2024-12-20T00:00:00+09:00",
            "date_modified": "2024-12-20T00:00:00+09:00",
            
                "author": 
                "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/11/07/on-python-udf-and-pyspark",
            "title": "On Python UDF and Pyspark",
            "summary": "This post delves into the intricacies of Python User-Defined Functions (UDFs) in PySpark, covering their mechanics, performance considerations, optimization techniques, and practical applications.",
            "content_text": "1. IntroductionPySpark empowers us to process large datasets efficiently. However, sometimes the built-in Spark functions aren’t enough, and that’s where User-Defined Functions (UDFs) come in handy. UDFs allow users to extend Spark’s capabilities by defining custom logic in Python. I want to explore how Python UDFs work in PySpark, focusing on their mechanics, performance, optimization, and best practices. By understanding the underlying principles, I could leverage Python’s expressive power while ensuring scalability in distributed computing environments.2. PySpark UDF Basics1) Creating UDFs1.  @udf DecoratorThis method uses a decorator to transform a regular Python function into a Spark UDF.from pyspark.sql.functions import udffrom pyspark.sql.types import StringType@udf(StringType())def my_upper(text):    return text.upper()# Usage with DataFrames:# df = ... (Load or create a DataFrame with a column named 'my_column')# df.select(my_upper(\"my_column\").alias(\"upper_column\"))2.  spark.udf.register FunctionThis method registers a Python function as a UDF, which can then be used in SQL queries or DataFrame transformations.from pyspark.sql import SparkSessionfrom pyspark.sql.types import IntegerTypespark = SparkSession.builder.appName(\"UDFExample\").getOrCreate()def add_five(num):    return num + 5spark.udf.register(\"addFive\", add_five, IntegerType())# Usage in SQL# df = ...# df.createOrReplaceTempView(\"my_table\")# spark.sql(\"SELECT addFive(my_number) FROM my_table\")2) Specifying Argument and Return TypesIt is crucial to specify the return type of your UDF using Spark SQL data types. This allows Spark to optimize data processing and avoid runtime errors. Common types include StringType(), IntegerType(), FloatType(), DateType(), and more.3. Performance and Limitations of PySpark UDFs1) Performance Issues with Python UDFs  Python Interpreter OverheadPython, being an interpreted language, generally has slower execution times compared to compiled languages. When PySpark uses a Python UDF, it involves starting the Python interpreter and executing the Python code for each row or a batch of rows. This incurs significant overhead.  Data Transfer CostsData needs to be transferred between the JVM (where Spark runs) and the Python interpreter. This process includes serialization (converting data into a byte stream for transmission) and deserialization (reconstructing the original data), which can be resource-intensive, especially when dealing with large datasets. This constant data movement between environments results in substantial performance penalties.2) Row-by-Row vs. Vectorized Operations  Row-by-Row OperationsTraditional Python UDFs typically operate on a row-by-row basis, meaning the Python function is executed individually for each row. This can lead to significant overhead when dealing with massive datasets because of the per-row function call overhead.  Vectorized OperationsVectorized operations, on the other hand, process data in batches or as entire columns, rather than processing row by row. This allows for optimized data processing, as these vectorized operations can leverage efficient, lower-level implementations (such as those provided by NumPy or Pandas).3) Impact on RDD and DataFrame Operation SpeedUsing Python UDFs can often slow down overall RDD and DataFrame operations. Built-in Spark functions are highly optimized and can often operate at the lower JVM level, which generally results in much faster operations. UDFs, however, break this optimization and introduce overhead related to the Python environment, reducing the potential benefits of using Spark.Performance issues with Python UDFs can often stem from several potential bottlenecks:  CPU Utilization: The Python interpreter and the UDF’s calculations can consume a considerable amount of CPU resources.  Memory Consumption: Data serialization and deserialization, and the creation of Python objects, can use significant memory, causing issues if not carefully managed.  Network I/O: Data transmission between JVM and Python processes across a network can become a bottleneck.4) Data Serialization and Deserialization ImpactThe process of converting data to a byte stream (serialization) before sending it to a Python process, and then reconstructing it into a JVM usable form (deserialization) can be time-consuming. Choosing the right serialization library, and optimizing data types, can reduce some of the overhead.When Spark Executors encounter Python UDFs, here’s the flow:  Data is transferred from the JVM to the Python environment.  The Python process executes the UDF logic.  The results are transferred back from the Python process to the JVM.This back-and-forth communication between two environments introduces significant overhead.5) Language Limitations: Python GIL (Global Interpreter Lock)Python’s Global Interpreter Lock (GIL) allows only one thread to execute Python bytecode at any given moment. This can limit the potential for parallelism within Python UDFs, even when multiple executors are available. While multiprocessing can be used as a workaround, this adds to the complexity of implementing and managing the PySpark code.4. Python UDF Optimization TechniquesGiven the performance limitations of standard Python UDFs, it’s crucial to employ optimization techniques to improve their efficiency. This section covers various strategies to enhance the performance of Python UDFs in PySpark.1) Vectorized UDFs with Pandas UDF  Working Principles and Advantages: Pandas UDFs utilize Apache Arrow, an in-memory columnar data format, for efficient data transfer between the JVM and Python processes. They allow data to be processed in batches, leveraging Pandas’ vectorized operations, which are significantly faster than iterating through individual rows.  pandas_udf Decorator and Function Definition: We create Pandas UDFs using the @pandas_udf decorator, which takes two arguments: the return type and the UDF type.      from pyspark.sql.functions import pandas_udf  from pyspark.sql.types import IntegerType, FloatType  import pandas as pd  # SCALAR type  @pandas_udf(FloatType(), functionType= \"scalar\")  def subtract_mean(series: pd.Series) -&gt; pd.Series:      return series - series.mean()      # GROUPED_MAP type  @pandas_udf(\"int\", \"grouped_map\")  def subtract_group_mean(pdf: pd.DataFrame) -&gt; pd.DataFrame:      pdf[\"value\"] = pdf[\"value\"] - pdf[\"value\"].mean()      return pdf    Here, functionType specifies the type of pandas UDF being defined.    GROUPED_MAP vs. SCALAR Types:          SCALAR: This type processes one or more pd.Series at a time and returns a pd.Series. It’s useful for applying transformations to columns. The UDF will receive one or more full columns as pd.Series.      GROUPED_MAP: This type processes one or more pd.DataFrame in group and returns a pd.DataFrame. It’s designed for operations that require the entire group’s data, such as calculations within groups. The UDF will receive a single group as a pd.DataFrame.        Performance Improvement with Pandas UDFsPandas UDFs can yield significant performance improvements compared to standard Python UDFs. Vectorized operations within Pandas are highly optimized, reducing overhead and allowing for faster processing of data.      from pyspark.sql import SparkSession  from pyspark.sql.functions import pandas_udf, col  from pyspark.sql.types import IntegerType  import pandas as pd      spark = SparkSession.builder.appName(\"PandasUDFExample\").getOrCreate()  data = [(1, 10), (1, 20), (2, 30), (2, 40), (3, 50)]  df = spark.createDataFrame(data, [\"group\", \"value\"])      @pandas_udf(IntegerType(), \"scalar\")  def add_one(s: pd.Series) -&gt; pd.Series:      return s + 1  df_transformed = df.withColumn(\"new_value\", add_one(col(\"value\")))  df_transformed.show()  # Output  # +-----+-----+---------+  # |group|value|new_value|  # +-----+-----+---------+  # |    1|   10|       11|  # |    1|   20|       21|  # |    2|   30|       31|  # |    2|   40|       41|  # |    3|   50|       51|  # +-----+-----+---------+    Performance Benefits and Use CasesPandas UDFs are especially beneficial when your UDF logic can be expressed using Pandas’ vectorized operations, such as arithmetic calculations, string operations, and data manipulation. They are suitable for tasks that require grouping and aggregations or data transformations within a group.2) Data Serialization/Deserialization Optimization  Selecting Serialization LibrariesWhile pickle is the default serialization method in Python, it’s not the most efficient, especially when dealing with large datasets. pyarrow is highly recommended for data serialization, especially when using Pandas UDFs, as it is designed for efficient columnar data formats and transfer between JVM and Python processes.  Data Type OptimizationChoose the smallest possible data type that can accommodate your data to reduce the size of the serialized data. Explicitly specify data types using Spark SQL types when defining UDFs.  Broadcasting Data for Efficient TransferIf your UDF requires static data, broadcasting it to executors can significantly reduce the amount of data that needs to be transferred. Broadcasting is a technique to share immutable data with all executors efficiently.",
            "content_html": "<h3 id=\"1-introduction\">1. Introduction</h3><p>PySpark empowers us to process large datasets efficiently. However, sometimes the built-in Spark functions aren’t enough, and that’s where User-Defined Functions (UDFs) come in handy. UDFs allow users to extend Spark’s capabilities by defining custom logic in Python. I want to explore how Python UDFs work in PySpark, focusing on their mechanics, performance, optimization, and best practices. By understanding the underlying principles, I could leverage Python’s expressive power while ensuring scalability in distributed computing environments.</p><h3 id=\"2-pyspark-udf-basics\">2. PySpark UDF Basics</h3><h4 id=\"1-creating-udfs\">1) Creating UDFs</h4><h5 id=\"1--udf-decorator\">1.  <code>@udf</code> Decorator</h5><p>This method uses a decorator to transform a regular Python function into a Spark UDF.</p><pre><code>from pyspark.sql.functions import udffrom pyspark.sql.types import StringType@udf(StringType())def my_upper(text):    return text.upper()# Usage with DataFrames:# df = ... (Load or create a DataFrame with a column named 'my_column')# df.select(my_upper(\"my_column\").alias(\"upper_column\"))</code></pre><h5 id=\"2--sparkudfregister-function\">2.  <code>spark.udf.register</code> Function</h5><p>This method registers a Python function as a UDF, which can then be used in SQL queries or DataFrame transformations.</p><pre><code>from pyspark.sql import SparkSessionfrom pyspark.sql.types import IntegerTypespark = SparkSession.builder.appName(\"UDFExample\").getOrCreate()def add_five(num):    return num + 5spark.udf.register(\"addFive\", add_five, IntegerType())# Usage in SQL# df = ...# df.createOrReplaceTempView(\"my_table\")# spark.sql(\"SELECT addFive(my_number) FROM my_table\")</code></pre><h4 id=\"2-specifying-argument-and-return-types\">2) Specifying Argument and Return Types</h4><p>It is crucial to specify the return type of your UDF using Spark SQL data types. This allows Spark to optimize data processing and avoid runtime errors. Common types include <code>StringType()</code>, <code>IntegerType()</code>, <code>FloatType()</code>, <code>DateType()</code>, and more.</p><h3 id=\"3-performance-and-limitations-of-pyspark-udfs\">3. Performance and Limitations of PySpark UDFs</h3><h4 id=\"1-performance-issues-with-python-udfs\">1) Performance Issues with Python UDFs</h4><ul>  <li>Python Interpreter Overhead<br />Python, being an interpreted language, generally has slower execution times compared to compiled languages. When PySpark uses a Python UDF, it involves starting the Python interpreter and executing the Python code for each row or a batch of rows. This incurs significant overhead.</li>  <li>Data Transfer Costs<br />Data needs to be transferred between the JVM (where Spark runs) and the Python interpreter. This process includes serialization (converting data into a byte stream for transmission) and deserialization (reconstructing the original data), which can be resource-intensive, especially when dealing with large datasets. This constant data movement between environments results in substantial performance penalties.</li></ul><h4 id=\"2-row-by-row-vs-vectorized-operations\">2) Row-by-Row vs. Vectorized Operations</h4><ul>  <li>Row-by-Row Operations<br />Traditional Python UDFs typically operate on a row-by-row basis, meaning the Python function is executed individually for each row. This can lead to significant overhead when dealing with massive datasets because of the per-row function call overhead.</li>  <li>Vectorized Operations<br />Vectorized operations, on the other hand, process data in batches or as entire columns, rather than processing row by row. This allows for optimized data processing, as these vectorized operations can leverage efficient, lower-level implementations (such as those provided by NumPy or Pandas).</li></ul><h4 id=\"3-impact-on-rdd-and-dataframe-operation-speed\">3) Impact on RDD and DataFrame Operation Speed</h4><p>Using Python UDFs can often slow down overall RDD and DataFrame operations. Built-in Spark functions are highly optimized and can often operate at the lower JVM level, which generally results in much faster operations. UDFs, however, break this optimization and introduce overhead related to the Python environment, reducing the potential benefits of using Spark.Performance issues with Python UDFs can often stem from several potential bottlenecks:</p><ul>  <li>CPU Utilization: The Python interpreter and the UDF’s calculations can consume a considerable amount of CPU resources.</li>  <li>Memory Consumption: Data serialization and deserialization, and the creation of Python objects, can use significant memory, causing issues if not carefully managed.</li>  <li>Network I/O: Data transmission between JVM and Python processes across a network can become a bottleneck.</li></ul><h4 id=\"4-data-serialization-and-deserialization-impact\">4) Data Serialization and Deserialization Impact</h4><p>The process of converting data to a byte stream (serialization) before sending it to a Python process, and then reconstructing it into a JVM usable form (deserialization) can be time-consuming. Choosing the right serialization library, and optimizing data types, can reduce some of the overhead.<br />When Spark Executors encounter Python UDFs, here’s the flow:</p><ol>  <li>Data is transferred from the JVM to the Python environment.</li>  <li>The Python process executes the UDF logic.</li>  <li>The results are transferred back from the Python process to the JVM.</li></ol><p>This back-and-forth communication between two environments introduces significant overhead.</p><h4 id=\"5-language-limitations-python-gil-global-interpreter-lock\">5) Language Limitations: Python GIL (Global Interpreter Lock)</h4><p>Python’s Global Interpreter Lock (GIL) allows only one thread to execute Python bytecode at any given moment. This can limit the potential for parallelism within Python UDFs, even when multiple executors are available. While multiprocessing can be used as a workaround, this adds to the complexity of implementing and managing the PySpark code.</p><h3 id=\"4-python-udf-optimization-techniques\">4. Python UDF Optimization Techniques</h3><p>Given the performance limitations of standard Python UDFs, it’s crucial to employ optimization techniques to improve their efficiency. This section covers various strategies to enhance the performance of Python UDFs in PySpark.</p><h4 id=\"1-vectorized-udfs-with-pandas-udf\">1) Vectorized UDFs with Pandas UDF</h4><ul>  <li>Working Principles and Advantages: Pandas UDFs utilize Apache Arrow, an in-memory columnar data format, for efficient data transfer between the JVM and Python processes. They allow data to be processed in batches, leveraging Pandas’ vectorized operations, which are significantly faster than iterating through individual rows.</li>  <li><code>pandas_udf</code> Decorator and Function Definition: We create Pandas UDFs using the <code>@pandas_udf</code> decorator, which takes two arguments: the return type and the UDF type.    <pre><code class=\"language-python\">  from pyspark.sql.functions import pandas_udf  from pyspark.sql.types import IntegerType, FloatType  import pandas as pd  # SCALAR type  @pandas_udf(FloatType(), functionType= \"scalar\")  def subtract_mean(series: pd.Series) -&gt; pd.Series:      return series - series.mean()      # GROUPED_MAP type  @pandas_udf(\"int\", \"grouped_map\")  def subtract_group_mean(pdf: pd.DataFrame) -&gt; pd.DataFrame:      pdf[\"value\"] = pdf[\"value\"] - pdf[\"value\"].mean()      return pdf</code></pre>    <p>Here, <code>functionType</code> specifies the type of pandas UDF being defined.</p>  </li>  <li><code>GROUPED_MAP</code> vs. <code>SCALAR</code> Types:    <ul>      <li><code>SCALAR</code>: This type processes one or more <code>pd.Series</code> at a time and returns a <code>pd.Series</code>. It’s useful for applying transformations to columns. The UDF will receive one or more full columns as <code>pd.Series</code>.</li>      <li><code>GROUPED_MAP</code>: This type processes one or more <code>pd.DataFrame</code> in group and returns a <code>pd.DataFrame</code>. It’s designed for operations that require the entire group’s data, such as calculations within groups. The UDF will receive a single group as a <code>pd.DataFrame</code>.</li>    </ul>  </li>  <li>Performance Improvement with Pandas UDFs<br />Pandas UDFs can yield significant performance improvements compared to standard Python UDFs. Vectorized operations within Pandas are highly optimized, reducing overhead and allowing for faster processing of data.    <pre><code class=\"language-python\">  from pyspark.sql import SparkSession  from pyspark.sql.functions import pandas_udf, col  from pyspark.sql.types import IntegerType  import pandas as pd      spark = SparkSession.builder.appName(\"PandasUDFExample\").getOrCreate()  data = [(1, 10), (1, 20), (2, 30), (2, 40), (3, 50)]  df = spark.createDataFrame(data, [\"group\", \"value\"])      @pandas_udf(IntegerType(), \"scalar\")  def add_one(s: pd.Series) -&gt; pd.Series:      return s + 1  df_transformed = df.withColumn(\"new_value\", add_one(col(\"value\")))  df_transformed.show()  # Output  # +-----+-----+---------+  # |group|value|new_value|  # +-----+-----+---------+  # |    1|   10|       11|  # |    1|   20|       21|  # |    2|   30|       31|  # |    2|   40|       41|  # |    3|   50|       51|  # +-----+-----+---------+</code></pre>  </li>  <li>Performance Benefits and Use Cases<br />Pandas UDFs are especially beneficial when your UDF logic can be expressed using Pandas’ vectorized operations, such as arithmetic calculations, string operations, and data manipulation. They are suitable for tasks that require grouping and aggregations or data transformations within a group.</li></ul><h4 id=\"2-data-serializationdeserialization-optimization\">2) Data Serialization/Deserialization Optimization</h4><ul>  <li>Selecting Serialization Libraries<br />While <code>pickle</code> is the default serialization method in Python, it’s not the most efficient, especially when dealing with large datasets. <code>pyarrow</code> is highly recommended for data serialization, especially when using Pandas UDFs, as it is designed for efficient columnar data formats and transfer between JVM and Python processes.</li>  <li>Data Type Optimization<br />Choose the smallest possible data type that can accommodate your data to reduce the size of the serialized data. Explicitly specify data types using Spark SQL types when defining UDFs.</li>  <li>Broadcasting Data for Efficient Transfer<br />If your UDF requires static data, broadcasting it to executors can significantly reduce the amount of data that needs to be transferred. Broadcasting is a technique to share immutable data with all executors efficiently.</li></ul>",
            "url": "http://localhost:4000/2024/11/07/on-python-udf-and-pyspark",
            
            
            
            
            
            "date_published": "2024-11-07T00:00:00+09:00",
            "date_modified": "2024-11-07T00:00:00+09:00",
            
                "author": 
                "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/07/05/what-happened-13th-century",
            "title": "What happened?: 13th Century",
            "summary": "A brief overview of significant events, trends, and developments during the 13th century.",
            "content_text": "1. Genghis Khan and the Mongol Empire  1162: Birth of Temujin  1180s-1206: Consolidation of Mongol tribes  1207-1210: Campaign against Western Xia  1211-1215: Invasion of the Jin Dynasty  1216-1218: Subjugation of the Kara-Khitai  1219-1225: Conquest of the Khwarazmian Empire  1226-1227: Final Campaign against Western Xia  1230s-1240s: Campaigns of Ögedei Khan  1250s-: Fragmentation of the Mongol Empire",
            "content_html": "<h3 id=\"1-genghis-khan-and-the-mongol-empire\">1. Genghis Khan and the Mongol Empire</h3><ul>  <li>1162: <a href=\"https://en.wikipedia.org/wiki/Genghis_Khan#Early_life\">Birth of Temujin</a></li>  <li>1180s-1206: <a href=\"https://en.wikipedia.org/wiki/Genghis_Khan#Rise_to_power\">Consolidation of Mongol tribes</a></li>  <li>1207-1210: <a href=\"https://en.wikipedia.org/wiki/Mongol_conquest_of_Western_Xia\">Campaign against Western Xia</a></li>  <li>1211-1215: <a href=\"https://en.wikipedia.org/wiki/Mongol%E2%80%93Jin_War\">Invasion of the Jin Dynasty</a></li>  <li>1216-1218: <a href=\"https://en.wikipedia.org/wiki/Mongol_conquest_of_the_Kara-Khitai\">Subjugation of the Kara-Khitai</a></li>  <li>1219-1225: <a href=\"https://en.wikipedia.org/wiki/Mongol_conquest_of_the_Khwarazmian_Empire\">Conquest of the Khwarazmian Empire</a></li>  <li>1226-1227: <a href=\"https://en.wikipedia.org/wiki/Mongol_conquest_of_Western_Xia#Final_conquest_of_Western_Xia\">Final Campaign against Western Xia</a></li>  <li>1230s-1240s: <a href=\"https://en.wikipedia.org/wiki/%C3%96gedei_Khan#Military_expansion\">Campaigns of Ögedei Khan</a></li>  <li>1250s-: <a href=\"https://en.wikipedia.org/wiki/Mongol_Empire#Fragmentation\">Fragmentation of the Mongol Empire</a></li></ul>",
            "url": "http://localhost:4000/2024/07/05/what-happened-13th-century",
            
            
            
            
            
            "date_published": "2024-07-05T00:00:00+09:00",
            "date_modified": "2024-07-05T00:00:00+09:00",
            
                "author": 
                "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}"
                
            
        }
    
    ]
}