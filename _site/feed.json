{
    "version": "https://jsonfeed.org/version/1",
    "title": "Sangmin Yoon",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": null,
    "icon": "http://localhost:4000/apple-touch-icon.png",
    "favicon": "http://localhost:4000/favicon.ico",
    "expired": false,
    
    "author": "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}",
    
"items": [
    
        {
            "id": "http://localhost:4000/2025/01/14/gpt-1-2-3-and-4",
            "title": "GPT 1, 2, 3 and 4",
            "summary": "What I learned while reading GPT 1, 2, 3 and 4 papers",
            "content_text": "1) GPT-1GPT-1 introduced a groundbreaking approach to natural language processing by leveraging the power of unsupervised pre-training. This technique involves training a language model on a massive amount of text data without any explicit labels. The resulting model, equipped with a deep understanding of language, can then be fine-tuned on specific tasks with relatively small labeled datasets.Key Differences from Traditional Supervised Learning  Pre-training: GPT-1’s approach starts with a pre-trained model, whereas traditional supervised learning trains a model from random initialization.  Data Efficiency: GPT-1 requires significantly less labeled data for fine-tuning, making it more practical for real-world applications.  Transfer Learning: GPT-1 enables transfer learning, allowing the model to adapt to new tasks with minimal effort.How does supervised fine-tuning work in GPT-1?  Pre-trained Model: A language model is first trained on a massive corpus of text data. This pre-training step allows the model to learn the statistical properties of language, such as grammar and semantics.  Task-Specific Fine-tuning: The pre-trained model is then adapted to a specific task using a labeled dataset. This involves adding a linear layer to the model and training it on the new task while keeping most of the pre-trained parameters fixed.  Input Transformations: To accommodate various tasks, GPT-1 employs input transformations. For instance, in question answering, the question, context, and answer choices are concatenated into a single sequence.  Objective Function: The model is trained to minimize a combined loss function that includes both a supervised loss for the specific task and an unsupervised loss to preserve the language modeling capabilities.2) GPT-2Key Advancements of GPT-2GPT-1 introduced the concept of pre-training a language model on a massive amount of text data and then fine-tuning it on specific tasks.GPT-2 took this concept to the next level by training an even larger model on a more diverse dataset.  Increased Model Size: GPT-2 employed significantly larger models, allowing for more complex pattern recognition and better generalization.  Unsupervised Multitask Learning: Unlike GPT-1, which relied on supervised fine-tuning for specific tasks, GPT-2 demonstrated the ability to perform a wide range of tasks directly from the pre-trained model, without explicit task-specific training.  Improved Zero-Shot Learning: GPT-2 exhibited remarkable zero-shot learning capabilities, meaning it could perform tasks it had not been explicitly trained on, simply by providing the task as text.3) GPT-3GPT-3 represents a significant leap forward in the evolution of large language models, building upon the successes of its predecessors, GPT-1 and GPT-2.Key Advancements of GPT-3  Massive Scale: GPT-3 boasts a significantly larger number of parameters compared to GPT-2, resulting in a dramatic increase in model capacity and computational power.  Enhanced Capabilities: GPT-3 demonstrated impressive abilities in various NLP tasks, including:          Text generation: Generating human-like text, writing stories, translating languages, and even composing poems.      Question answering: Providing comprehensive and informative answers to a wide range of questions.      Code generation: Writing and debugging code in various programming languages.      Creative content creation: Generating novel content, such as scripts, articles, and even musical pieces.        In-context learning: GPT-3 exhibited remarkable in-context learning capabilities, demonstrating the ability to adapt to new tasks or concepts simply by providing a few examples within the input.  Few-shot learning: GPT-3 achieved impressive results in few-shot learning scenarios, where the model is provided with only a few labeled examples for a new task.4) GPT-4GPT-4 represents the latest advancement in the GPT series of large language models, pushing the boundaries of AI capabilities even further.Key Advancements of GPT-4  Enhanced Capabilities: GPT-4 demonstrates significantly improved performance across a wide range of tasks, including:          Creativity: Generating more creative and innovative text formats, such as poems, code, and scripts.      Problem-solving: Exhibiting enhanced reasoning and problem-solving abilities, including the ability to handle more complex and nuanced tasks.      Safety and Reliability: GPT-4 has been developed with a strong emphasis on safety and reliability, aiming to minimize biases, hallucinations, and harmful outputs.        Multimodality: GPT-4 introduces multimodality, allowing it to accept and process both text and image inputs. This opens up new possibilities for applications such as image captioning, visual question answering, and more.  Advanced In-context Learning: GPT-4 further enhances in-context learning capabilities, allowing it to learn and adapt to new tasks with even fewer examples.",
            "content_html": "<h3 id=\"1-gpt-1\">1) GPT-1</h3><p>GPT-1 introduced a groundbreaking approach to natural language processing by leveraging the power of unsupervised pre-training. This technique involves training a language model on a massive amount of text data without any explicit labels. The resulting model, equipped with a deep understanding of language, can then be fine-tuned on specific tasks with relatively small labeled datasets.</p><h4 id=\"key-differences-from-traditional-supervised-learning\">Key Differences from Traditional Supervised Learning</h4><ul>  <li>Pre-training: GPT-1’s approach starts with a pre-trained model, whereas traditional supervised learning trains a model from random initialization.</li>  <li>Data Efficiency: GPT-1 requires significantly less labeled data for fine-tuning, making it more practical for real-world applications.</li>  <li>Transfer Learning: GPT-1 enables transfer learning, allowing the model to adapt to new tasks with minimal effort.</li></ul><h4 id=\"how-does-supervised-fine-tuning-work-in-gpt-1\">How does supervised fine-tuning work in GPT-1?</h4><ul>  <li>Pre-trained Model: A language model is first trained on a massive corpus of text data. This pre-training step allows the model to learn the statistical properties of language, such as grammar and semantics.</li>  <li>Task-Specific Fine-tuning: The pre-trained model is then adapted to a specific task using a labeled dataset. This involves adding a linear layer to the model and training it on the new task while keeping most of the pre-trained parameters fixed.</li>  <li>Input Transformations: To accommodate various tasks, GPT-1 employs input transformations. For instance, in question answering, the question, context, and answer choices are concatenated into a single sequence.</li>  <li>Objective Function: The model is trained to minimize a combined loss function that includes both a supervised loss for the specific task and an unsupervised loss to preserve the language modeling capabilities.</li></ul><p><br /></p><h3 id=\"2-gpt-2\">2) GPT-2</h3><h4 id=\"key-advancements-of-gpt-2\">Key Advancements of GPT-2</h4><p>GPT-1 introduced the concept of pre-training a language model on a massive amount of text data and then fine-tuning it on specific tasks.GPT-2 took this concept to the next level by training an even larger model on a more diverse dataset.</p><ul>  <li>Increased Model Size: GPT-2 employed significantly larger models, allowing for more complex pattern recognition and better generalization.</li>  <li>Unsupervised Multitask Learning: Unlike GPT-1, which relied on supervised fine-tuning for specific tasks, GPT-2 demonstrated the ability to perform a wide range of tasks directly from the pre-trained model, without explicit task-specific training.</li>  <li>Improved Zero-Shot Learning: GPT-2 exhibited remarkable zero-shot learning capabilities, meaning it could perform tasks it had not been explicitly trained on, simply by providing the task as text.</li></ul><p><br /></p><h3 id=\"3-gpt-3\">3) GPT-3</h3><p>GPT-3 represents a significant leap forward in the evolution of large language models, building upon the successes of its predecessors, GPT-1 and GPT-2.</p><h4 id=\"key-advancements-of-gpt-3\">Key Advancements of GPT-3</h4><ul>  <li>Massive Scale: GPT-3 boasts a significantly larger number of parameters compared to GPT-2, resulting in a dramatic increase in model capacity and computational power.</li>  <li>Enhanced Capabilities: GPT-3 demonstrated impressive abilities in various NLP tasks, including:    <ul>      <li>Text generation: Generating human-like text, writing stories, translating languages, and even composing poems.</li>      <li>Question answering: Providing comprehensive and informative answers to a wide range of questions.</li>      <li>Code generation: Writing and debugging code in various programming languages.</li>      <li>Creative content creation: Generating novel content, such as scripts, articles, and even musical pieces.</li>    </ul>  </li>  <li>In-context learning: GPT-3 exhibited remarkable in-context learning capabilities, demonstrating the ability to adapt to new tasks or concepts simply by providing a few examples within the input.</li>  <li>Few-shot learning: GPT-3 achieved impressive results in few-shot learning scenarios, where the model is provided with only a few labeled examples for a new task.</li></ul><p><br /></p><h3 id=\"4-gpt-4\">4) GPT-4</h3><p>GPT-4 represents the latest advancement in the GPT series of large language models, pushing the boundaries of AI capabilities even further.</p><h4 id=\"key-advancements-of-gpt-4\">Key Advancements of GPT-4</h4><ul>  <li>Enhanced Capabilities: GPT-4 demonstrates significantly improved performance across a wide range of tasks, including:    <ul>      <li>Creativity: Generating more creative and innovative text formats, such as poems, code, and scripts.</li>      <li>Problem-solving: Exhibiting enhanced reasoning and problem-solving abilities, including the ability to handle more complex and nuanced tasks.</li>      <li>Safety and Reliability: GPT-4 has been developed with a strong emphasis on safety and reliability, aiming to minimize biases, hallucinations, and harmful outputs.</li>    </ul>  </li>  <li>Multimodality: GPT-4 introduces multimodality, allowing it to accept and process both text and image inputs. This opens up new possibilities for applications such as image captioning, visual question answering, and more.</li>  <li>Advanced In-context Learning: GPT-4 further enhances in-context learning capabilities, allowing it to learn and adapt to new tasks with even fewer examples.</li></ul>",
            "url": "http://localhost:4000/2025/01/14/gpt-1-2-3-and-4",
            
            
            
            
            
            "date_published": "2025-01-14T00:00:00+09:00",
            "date_modified": "2025-01-14T00:00:00+09:00",
            
                "author": 
                "{"twitter" => nil, "name" => nil, "avatar" => nil, "email" => nil, "url" => nil}"
                
            
        }
    
    ]
}